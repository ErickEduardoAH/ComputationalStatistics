{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='static/spark.png' width=\"30%\" height=\"30%\" />\n",
    "\n",
    "### Introducci√≥n a spark.\n",
    "\n",
    "Apache Spark is a general purpose distributed computing platform scalable, designed to support processing large volumes of data. Spark extends the mapping-reduction model by efficiently implementing calculations in RAM. Spark components are a series of components in their core, which is the \"computer engine\" responsible for scheduling, distributing, and monitoring applications which consist of a series of tasks that are distributed along units of processing that execute distributed code called (workers), the workers reside in a physical machine that is integrated to the cluster, a machine of the cluster can contain one or several workers.\n",
    "\n",
    "\n",
    "<img src='static/driver.png' width=\"30%\" height=\"30%\" />\n",
    "\n",
    "Each spark application consists of a diver program which is the main program in which both distributed and local structures and operations are defined. In the driver program the flow and logic of the application to be executed reside. The driver accesses the cluster through a SparkContext object which represents the physical connection to the cluster and thus is the object with which to create distributed data structures as a first instance, this SparkContext object is assigned to the driver by the master when Performs the submit of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9caa397debb3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient distributed datasets\n",
    "\n",
    "Resilient distributed datasets or (RDD's) is a distributed data structure composed of an immutable collection of objects, this collection is divided into sub-sets called partitions, each partition is hosted in a cluster worker. On this collection of objects it is only possible to perform distributed tasks provided by the Spark core, so that all the work defined in the driver program flow on these data structures will be automatically parellizado and applied on the processing units that Partitions of the RDD to operate. These collections of elements can be stored in RAM or disk, and may contain primitive data of java, scala and python, or even classes defined by the user or own classes of the spark core.\n",
    "\n",
    "<img src='static/rdd.png' width=\"50%\" height=\"50%\"/>\n",
    "\n",
    "<center>*RDD separado en 5 particiones, algunas de ellas residen en el mismo worker*</center>\n",
    "\n",
    "Some considerations about this data structure are:\n",
    "* RDDs are automatically rebuilt when a machine failure occurs in some processing unit.\n",
    "* RDD's can be created from parallelizing some local language data structure (lists, arrays, matrices, etc ...) through the SparkContext with the method paralelize (local_local) or when applying certain distributed transformation operations to From another RDD (or some data structure of lower hierarchy in the sense of inheritance).\n",
    "* Due to the resilience these are immutable, ie it is not possible to modify them once created. When we transform a RDD we are really creating a new one.\n",
    "\n",
    "**Example**: Creating a RDD from a list of numbers through SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:175"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miFirstRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25])\n",
    "miFirstRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the elements of the RDD are distributed in the cluster it is not possible to access them in a sequential way, since the RDD functions as a generic pointer to an abstract collection of homogeneous elements, so an instruction of type 'object = RDD [ I] 'is meaningless in this context, however there are RDD functions that allow elements of the rdd to be brought into the physical memory of the local machine:\n",
    "\n",
    "* **Collect ()**: This function collects all the objects of the rdd that are distributed throughout the cluster and stores them in the driver's local memory in the form of a sequential list, the indiscriminate use of this function in distributed environments with high volumetry can ocacionar a Machine failure in the master because the volume of the data can exceed by far the physical capacities of the machine where the driver is executed.\n",
    "\n",
    "* **Take (small number)**: This function collects an amount of n arbitrary elements of the distributed dataset and similarly accommodates in local memory. This function is not deterministic so you may get a different set on the data set when you run it several times.\n",
    "\n",
    "**Example**: Obtaining items from RDD to local memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3],\n",
       " [4, 5, 6],\n",
       " [7, 8, 9],\n",
       " [10, 11, 12],\n",
       " [13, 14, 15],\n",
       " [16, 17, 18],\n",
       " [19, 20, 21],\n",
       " [22, 23, 24, 25]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miFirstRDD.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "Take: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print('Collect: '+str(miFirstRDD.collect()))\n",
    "print('Take: '+str(miFirstRDD.take(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Creating a RDD through a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take: ['594.38', '281.37', '144.85', '20.99', '797.40']\n"
     ]
    }
   ],
   "source": [
    "numbersRDD = sc.textFile('data/amounts.txt')\n",
    "print('Take: '+str(numbersRDD.take(5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
