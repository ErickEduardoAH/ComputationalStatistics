{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames and Relational Operations\n",
    "\n",
    "A DataFrame is a RDD organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The form to create dataFrames in spark is through a SQL context wich is an object of the Spark SQL module.\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "     <img src=\"static/sparkDataframe.png\" width=\"70%\">\n",
    "</div>\n",
    "\n",
    "### Creating dataFrames with SQLContext\n",
    "\n",
    "The SqlContext is a class and is used for initializing the functionalities of Spark SQL. SparkContext class object (sc) is required for initializing SQLContext class object. When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created in the following ways:\n",
    "\n",
    "1) Apply the schema to the RDD of Rows via **createDataFrame** method provided by **SparkContext**.\n",
    "\n",
    "**Example:** Creating a the SQL context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Creating a dataframe using the example of the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervalClass(value):\n",
    "    interval = math.floor((value-minimum)/intervalLength)\n",
    "    if (interval >= bins):\n",
    "        return bins-1\n",
    "    else:\n",
    "        return interval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "floatsRDD = sc.textFile('data/amounts.txt').map(lambda x: float(x))\n",
    "bins = 30\n",
    "theCountOfTheRDD=floatsRDD.count()\n",
    "maximum = floatsRDD.max()\n",
    "minimum = floatsRDD.min()\n",
    "intervalLength = (maximum-minimum)/bins\n",
    "frequencyTableRDD = floatsRDD.map(lambda x:(intervalClass(x),1))\\\n",
    "                             .reduceByKey(lambda x,y: x+y)\\\n",
    "                             .map(lambda y:(y[0]+1,\\\n",
    "                                  minimum+intervalLength*y[0],\\\n",
    "                                  minimum+intervalLength*(y[0]+1),\\\n",
    "                                  y[1]/theCountOfTheRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+---------+\n",
      "|Bin|        LowerLimit|        UpperLimit|Frequency|\n",
      "+---+------------------+------------------+---------+\n",
      "|  1|              0.06|152.11999999999998|   0.2662|\n",
      "|  3|304.17999999999995|456.23999999999995|   0.1424|\n",
      "|  5| 608.2999999999998| 760.3599999999998|   0.0764|\n",
      "| 15|2128.8999999999996|2280.9599999999996|   0.0046|\n",
      "|  9|1216.5399999999997|1368.5999999999997|   0.0232|\n",
      "| 13|1824.7799999999997|1976.8399999999997|   0.0058|\n",
      "|  7| 912.4199999999998|1064.4799999999998|    0.041|\n",
      "| 21|3041.2599999999993|3193.3199999999993|   8.0E-4|\n",
      "| 11|1520.6599999999996|1672.7199999999996|   0.0156|\n",
      "| 17|2433.0199999999995|2585.0799999999995|   0.0022|\n",
      "| 23| 3345.379999999999| 3497.439999999999|   2.0E-4|\n",
      "| 29|           4257.74| 4409.799999999999|   4.0E-4|\n",
      "| 19|2737.1399999999994|2889.1999999999994|   0.0014|\n",
      "|  4|456.23999999999995| 608.2999999999998|    0.108|\n",
      "|  2|152.11999999999998|304.17999999999995|   0.1954|\n",
      "|  6| 760.3599999999998| 912.4199999999998|   0.0516|\n",
      "|  8|1064.4799999999998|1216.5399999999997|    0.028|\n",
      "| 10|1368.5999999999997|1520.6599999999996|   0.0166|\n",
      "| 12|1672.7199999999996|1824.7799999999997|   0.0112|\n",
      "| 14|1976.8399999999997|2128.8999999999996|   0.0046|\n",
      "+---+------------------+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frequencyTableSchema=['Bin','LowerLimit','UpperLimit','Frequency']\n",
    "frequencyDF = sqlContext.createDataFrame(frequencyTableRDD,frequencyTableSchema)\n",
    "frequencyDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Create an RDD of tuples or lists from the original RDD.\n",
    "\n",
    "**Example**: Creating a DataFrame using the function **toDF( )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Bin_|      AverageByBin|\n",
      "+----+------------------+\n",
      "|   0| 72.45056348610062|\n",
      "|   2|372.61179775280897|\n",
      "|   4| 681.9975130890053|\n",
      "|  14|2207.4121739130433|\n",
      "|   8|1291.9476724137935|\n",
      "|  12| 1893.831724137931|\n",
      "|   6| 984.4356585365854|\n",
      "|  20|         3111.7075|\n",
      "|  10|1595.1253846153845|\n",
      "|  16|2490.8645454545454|\n",
      "|  22|           3484.16|\n",
      "|  28|           4317.21|\n",
      "|  18|2781.6885714285713|\n",
      "|   3| 529.4409444444444|\n",
      "|   1| 224.4947697031729|\n",
      "|   5| 833.8509302325581|\n",
      "|   7|1133.4300714285712|\n",
      "|   9|1440.3262650602408|\n",
      "|  11|1747.8194642857145|\n",
      "|  13|2036.4704347826087|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meansTableSchema = ['Bin_','AverageByBin']\n",
    "meansTableDF = floatsRDD.map(lambda x:(intervalClass(x),(x,1)))\\\n",
    "                        .reduceByKey(lambda x, y: (x[0]+y[0],x[1]+y[1]))\\\n",
    "                        .map(lambda z: (z[0],z[1][0]/z[1][1]))\\\n",
    "                        .toDF(meansTableSchema)\n",
    "meansTableDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Bin: long (nullable = true)\n",
      " |-- LowerLimit: double (nullable = true)\n",
      " |-- UpperLimit: double (nullable = true)\n",
      " |-- Frequency: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Bin_: long (nullable = true)\n",
      " |-- AverageByBin: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frequencyDF.printSchema()\n",
    "meansTableDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation with SQL operators\n",
    "---\n",
    "Due to SQL is the most popular query language for data manipulation, spark DataFrames provides a set of relational operations in addition to the mapreduce operations inherit of the spark rdd.\n",
    "\n",
    "\n",
    "### Select and where clauses\n",
    "\n",
    "**Example:** Loadings data using the spark SQL context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "|User_id|Firstname| Lastname|\n",
      "+-------+---------+---------+\n",
      "|      1|  Timothy|   Snyder|\n",
      "|      2|    Scott|    Jones|\n",
      "|      3|    Debra|      Kim|\n",
      "|      4|    Steve|   Austin|\n",
      "|      5|   Andrew|   Chavez|\n",
      "|      6|   Philip|  Nichols|\n",
      "|      7| Nicholas|    Price|\n",
      "|      8|   Angela| Lawrence|\n",
      "|      9|    Alice|   Martin|\n",
      "|     10|    Kathy|    Davis|\n",
      "|     11|  Jeffrey|     Wood|\n",
      "|     12|   Martha|   Howard|\n",
      "|     13|  Anthony|    Henry|\n",
      "|     14|     Rose|  Stewart|\n",
      "|     15|   Nicole|   Bishop|\n",
      "|     16|    Linda|   Weaver|\n",
      "|     17| Michelle|   Bowman|\n",
      "|     18|   Johnny|Henderson|\n",
      "|     19|     Sean|    Smith|\n",
      "|     20|   Sandra|  Garrett|\n",
      "+-------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------------+\n",
      "|Movie_id|              Title|\n",
      "+--------+-------------------+\n",
      "|       1|          Toy Story|\n",
      "|       2|          GoldenEye|\n",
      "|       3|         Four Rooms|\n",
      "|       4|         Get Shorty|\n",
      "|       5|            Copycat|\n",
      "|       6|    Shanghai Triad |\n",
      "|       7|     Twelve Monkeys|\n",
      "|       8|               Babe|\n",
      "|       9|   Dead Man Walking|\n",
      "|      10|        Richard III|\n",
      "|      11|             Seven |\n",
      "|      12|     Usual Suspects|\n",
      "|      13|   Mighty Aphrodite|\n",
      "|      14|            Postino|\n",
      "|      15| Mr. Holland's Opus|\n",
      "|      16|      French Twist |\n",
      "|      17|From Dusk Till Dawn|\n",
      "|      18|      White Balloon|\n",
      "|      19|     Antonia's Line|\n",
      "|      20| Angels and Insects|\n",
      "+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------+------+\n",
      "|Movie_id|User_id|Rating|\n",
      "+--------+-------+------+\n",
      "|       1|      6|     5|\n",
      "|       1|     10|     3|\n",
      "|       1|     12|     5|\n",
      "|       1|     14|     5|\n",
      "|       1|     17|     3|\n",
      "|       1|     20|     4|\n",
      "|       1|     23|     4|\n",
      "|       1|     24|     3|\n",
      "|       1|     27|     2|\n",
      "|       1|     31|     3|\n",
      "|       1|     33|     4|\n",
      "|       1|     36|     2|\n",
      "|       1|     39|     4|\n",
      "|       1|     44|     5|\n",
      "|       1|     47|     4|\n",
      "|       1|     49|     3|\n",
      "|       1|     51|     4|\n",
      "|       1|     53|     3|\n",
      "|       1|     54|     3|\n",
      "|       1|     56|     4|\n",
      "+--------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "moviesCatalogDF = sqlContext.createDataFrame(pd.read_csv('data/MoviesCatalog.csv'))\n",
    "userCatalogDF = sqlContext.createDataFrame(pd.read_csv('data/UserCatalog.csv'))\n",
    "moviesRatingsDF = sqlContext.createDataFrame(pd.read_csv('data/MoviesRatings.csv'))\n",
    "userCatalogDF.show()\n",
    "moviesCatalogDF.show()\n",
    "moviesRatingsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join clause\n",
    "\n",
    "The **join** consists of combining each row of a table with each row of the other table, selecting those rows that meet a certain condition.\n",
    "\n",
    "**Example:** Joining the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+---------+------+-----------------+\n",
      "|Movie_id|User_id|Firstname| Lastname|Rating|            Title|\n",
      "+--------+-------+---------+---------+------+-----------------+\n",
      "|      26|    222|   Harold|   Jacobs|     3|Brothers McMullen|\n",
      "|      26|    293| Michelle|  Garrett|     3|Brothers McMullen|\n",
      "|      26|    237|    Bruce|  Stewart|     3|Brothers McMullen|\n",
      "|      26|      7| Nicholas|    Price|     3|Brothers McMullen|\n",
      "|      26|    126|    Roger|   Nelson|     4|Brothers McMullen|\n",
      "|      26|    274|   Willie|Armstrong|     3|Brothers McMullen|\n",
      "|      26|     50|    Chris|    Owens|     4|Brothers McMullen|\n",
      "|      26|    343|   Joseph|   Gordon|     3|Brothers McMullen|\n",
      "|      26|    116| Lawrence|    Mason|     2|Brothers McMullen|\n",
      "|      26|     25|   Dennis|    Reyes|     3|Brothers McMullen|\n",
      "|      26|    181|   Steven|  Spencer|     4|Brothers McMullen|\n",
      "|      26|    369|    Linda|    Woods|     2|Brothers McMullen|\n",
      "|      26|    413|     Juan|      Lee|     2|Brothers McMullen|\n",
      "|      26|    455|    Randy|   Hunter|     3|Brothers McMullen|\n",
      "|      26|    255|   Teresa|   Oliver|     3|Brothers McMullen|\n",
      "|      26|    258|   Daniel|     Ryan|     3|Brothers McMullen|\n",
      "|      26|    271|    Sarah|  Spencer|     3|Brothers McMullen|\n",
      "|      26|      9|    Alice|   Martin|     4|Brothers McMullen|\n",
      "|      26|    316|  Anthony|    Reyes|     3|Brothers McMullen|\n",
      "|      26|    150|    Randy|  Sanchez|     3|Brothers McMullen|\n",
      "+--------+-------+---------+---------+------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviesUserInfoDF = userCatalogDF.join(moviesRatingsDF,on='User_id')\\\n",
    "                                .join(moviesCatalogDF,on='Movie_id')\\\n",
    "    \n",
    "moviesUserInfoDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Joining the catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+---------+----+------------------+\n",
      "|Bin|        LowerLimit|        UpperLimit|Frequency|Bin_|      AverageByBin|\n",
      "+---+------------------+------------------+---------+----+------------------+\n",
      "| 29|           4257.74| 4409.799999999999|   4.0E-4|  29|           4561.86|\n",
      "| 19|2737.1399999999994|2889.1999999999994|   0.0014|  19|          2913.605|\n",
      "|  7| 912.4199999999998|1064.4799999999998|    0.041|   7|1133.4300714285712|\n",
      "|  6| 760.3599999999998| 912.4199999999998|   0.0516|   6| 984.4356585365854|\n",
      "|  9|1216.5399999999997|1368.5999999999997|   0.0232|   9|1440.3262650602408|\n",
      "| 17|2433.0199999999995|2585.0799999999995|   0.0022|  17|         2641.0675|\n",
      "|  5| 608.2999999999998| 760.3599999999998|   0.0764|   5| 833.8509302325581|\n",
      "|  1|              0.06|152.11999999999998|   0.2662|   1| 224.4947697031729|\n",
      "| 10|1368.5999999999997|1520.6599999999996|   0.0166|  10|1595.1253846153845|\n",
      "|  3|304.17999999999995|456.23999999999995|   0.1424|   3| 529.4409444444444|\n",
      "| 12|1672.7199999999996|1824.7799999999997|   0.0112|  12| 1893.831724137931|\n",
      "|  8|1064.4799999999998|1216.5399999999997|    0.028|   8|1291.9476724137935|\n",
      "| 11|1520.6599999999996|1672.7199999999996|   0.0156|  11|1747.8194642857145|\n",
      "|  2|152.11999999999998|304.17999999999995|   0.1954|   2|372.61179775280897|\n",
      "|  4|456.23999999999995| 608.2999999999998|    0.108|   4| 681.9975130890053|\n",
      "| 13|1824.7799999999997|1976.8399999999997|   0.0058|  13|2036.4704347826087|\n",
      "| 18|2585.0799999999995|2737.1399999999994|   8.0E-4|  18|2781.6885714285713|\n",
      "| 14|1976.8399999999997|2128.8999999999996|   0.0046|  14|2207.4121739130433|\n",
      "| 15|2128.8999999999996|2280.9599999999996|   0.0046|  15| 2357.843846153846|\n",
      "| 20|2889.1999999999994|3041.2599999999993|   4.0E-4|  20|         3111.7075|\n",
      "+---+------------------+------------------+---------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "frequencyDF.join(meansTableDF,col('Bin')==col('Bin_')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and where clauses\n",
    "\n",
    "**Querry**: Obtain the movies rated by Kevin Scott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+--------------------+\n",
      "|Firstname|Lastname|Rating|               Title|\n",
      "+---------+--------+------+--------------------+\n",
      "|    Kevin|   Scott|     5|   Brothers McMullen|\n",
      "|    Kevin|   Scott|     4|      Batman Forever|\n",
      "|    Kevin|   Scott|     4|             Amadeus|\n",
      "|    Kevin|   Scott|     3|Star Trek: First ...|\n",
      "|    Kevin|   Scott|     4|            Outbreak|\n",
      "|    Kevin|   Scott|     4|         Restoration|\n",
      "|    Kevin|   Scott|     4|             Flipper|\n",
      "|    Kevin|   Scott|     3|Last of the Mohicans|\n",
      "|    Kevin|   Scott|     4|To Kill a Mocking...|\n",
      "|    Kevin|   Scott|     4|      Spitfire Grill|\n",
      "|    Kevin|   Scott|     4|          Home Alone|\n",
      "|    Kevin|   Scott|     5|Star Trek III: Th...|\n",
      "|    Kevin|   Scott|     5|             Henry V|\n",
      "|    Kevin|   Scott|     3|            Die Hard|\n",
      "|    Kevin|   Scott|     4|          Disclosure|\n",
      "|    Kevin|   Scott|     4| Alice in Wonderland|\n",
      "|    Kevin|   Scott|     5|Miracle on 34th S...|\n",
      "|    Kevin|   Scott|     4|        Crimson Tide|\n",
      "|    Kevin|   Scott|     5|        Strange Days|\n",
      "|    Kevin|   Scott|     4|        Evil Dead II|\n",
      "+---------+--------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviesUserInfoDF.select(['Firstname','Lastname','Rating','Title'])\\\n",
    "                .where((col('Firstname')=='Kevin') & (col('Lastname')=='Scott'))\\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Querry**: Obtain the movies with rate greather than 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+-----------------+\n",
      "|Firstname| Lastname|Rating|            Title|\n",
      "+---------+---------+------+-----------------+\n",
      "|   Harold|   Jacobs|     3|Brothers McMullen|\n",
      "| Michelle|  Garrett|     3|Brothers McMullen|\n",
      "|    Bruce|  Stewart|     3|Brothers McMullen|\n",
      "| Nicholas|    Price|     3|Brothers McMullen|\n",
      "|    Roger|   Nelson|     4|Brothers McMullen|\n",
      "|   Willie|Armstrong|     3|Brothers McMullen|\n",
      "|    Chris|    Owens|     4|Brothers McMullen|\n",
      "|   Joseph|   Gordon|     3|Brothers McMullen|\n",
      "|   Dennis|    Reyes|     3|Brothers McMullen|\n",
      "|   Steven|  Spencer|     4|Brothers McMullen|\n",
      "|    Randy|   Hunter|     3|Brothers McMullen|\n",
      "|   Teresa|   Oliver|     3|Brothers McMullen|\n",
      "|   Daniel|     Ryan|     3|Brothers McMullen|\n",
      "|    Sarah|  Spencer|     3|Brothers McMullen|\n",
      "|    Alice|   Martin|     4|Brothers McMullen|\n",
      "|  Anthony|    Reyes|     3|Brothers McMullen|\n",
      "|    Randy|  Sanchez|     3|Brothers McMullen|\n",
      "|    Jason| Johnston|     4|Brothers McMullen|\n",
      "|    Terry|   Harvey|     4|Brothers McMullen|\n",
      "|   Ashley|    Smith|     3|Brothers McMullen|\n",
      "+---------+---------+------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviesUserInfoDF.select(['Firstname','Lastname','Rating','Title'])\\\n",
    "                .where(col('Rating')>=3)\\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy clause and agregation functions over groups\n",
    "\n",
    "\n",
    "**Querry**: Obtain the top 10 rated movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|           Title|      avg(Rating)|\n",
      "+----------------+-----------------+\n",
      "|    Mary Poppins|              5.0|\n",
      "|     Black Sheep|              5.0|\n",
      "|            Blob|              5.0|\n",
      "|  Sound of Music|             4.75|\n",
      "|         Twister|4.714285714285714|\n",
      "|             187|4.695652173913044|\n",
      "|  Amityville 3-D|4.666666666666667|\n",
      "|Dead Man Walking|           4.5625|\n",
      "|       Big Night|             4.55|\n",
      "|  Princess Bride|4.545454545454546|\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviesUserInfoDF.select(['Title','Rating'])\\\n",
    "                .groupBy('Title').avg()\\\n",
    "                .orderBy('avg(Rating)',ascending= False)\\\n",
    "                .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introductions to SQL Functions\n",
    "___\n",
    "**UDF: ** User defined functions are a simple way of adding a function into the SparkSQL language. This function operates on distributed DataFrames and works row by row (unless you're creating an user defined aggregation function).\n",
    "\n",
    "**Example**: Transforming from string to data type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only seen types of SQL functions that act on groups and return a single value for each group, ie aggregation functions, however, they are quite useful in practice, there is still a wide range of operations that cannot be expressed using these types of functions alone. Specifically, there was no way to both operate on a group of rows while still returning a single value for every input row. This limitation makes it hard to conduct various data processing tasks like calculating a moving average, calculating a cumulative sum, or accessing the values of a row appearing before the current row. \n",
    "\n",
    "Fortunately for users of Spark SQL, window functions fill this gap. At its core, a window function calculates a return value for every input row of a table based on a group of rows, called the Frame. Every input row can have a unique frame associated with it. This characteristic of window functions makes them more powerful than other functions and allows users to express various data processing tasks that are hard (if not impossible) to be expressed without window functions in a concise way.\n",
    "\n",
    "**Windows functions:** Windowed Aggregates (window functions) are functions that perform a calculation on a group of records (called a window) that are in some relation to the current record. Window functions calculate values for every record in a window. For specifying a sliding window, there are three components: \n",
    "\n",
    "\n",
    "* **Partition by:** Defines how the data is grouped; in the above example, it was by customer. You have to specify a reasonable grouping because all data within a group will be collected to the same machine. Ideally, the DataFrame has already been partitioned by the desired grouping.\n",
    "\n",
    "* **Order by**: Defines how rows are ordered within a group; in the above example, it was by date.\n",
    "\n",
    "* **Frame** Defines the boundaries of the window with respect to the current row; in the above example, the window ranged between the previous row and the next row.\n",
    "\n",
    "\n",
    "**Example**: The moving average for each card at the time of each transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Cumulated sum by card at time of each transaction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Adding an increasing id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
